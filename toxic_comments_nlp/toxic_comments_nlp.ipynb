{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Определение токсичных комментариев для английских текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Нужно обучить модель классифицировать комментарии на позитивные и негативные. Для этой задачи создан набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Нужно построить модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Шаги**\n",
    "\n",
    "1. Загрузка и подготовка данных.\n",
    "2. Обучение разных моделей. \n",
    "3. Выводы.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. \n",
    "\n",
    "Столбец *text* — текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pandas==1.4.4\n",
    "numpy==1.21.5\n",
    "matplotlib==3.5.2\n",
    "scikit-learn==1.0.2\n",
    "lightgbm==3.3.5\n",
    "nltk==3.7\n",
    "imbalanced-learn==0.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка, подготовка и анализ данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка данных и их предварительный просмотр:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    data = pd.read_csv('D:\\\\data\\\\toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть странная колонка \"Unnamed: 0\". С первого взгляда кажется, что она дублирует индексы. Проверю, так ли это:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index.equals(data['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нет, дублирования нет. Проверю, где значения начинают отличаться друг от друга (номер индекса):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6080"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, (idx, value) in enumerate(data['Unnamed: 0'].items()):\n",
    "    if idx != value:\n",
    "        first_mismatch_index = idx\n",
    "        break\n",
    "        \n",
    "first_mismatch_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6078</th>\n",
       "      <td>6078</td>\n",
       "      <td>\"\\n\\nPenis envy\\nFrom Wikipedia, the free ency...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6079</th>\n",
       "      <td>6079</td>\n",
       "      <td>Hrafn \\n\\nPM, Where did Hrafn go? There was a ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6080</th>\n",
       "      <td>6084</td>\n",
       "      <td>\"::I'll alos be looking in to see how this is ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6081</th>\n",
       "      <td>6085</td>\n",
       "      <td>\"\\n\\nThe Ezekiel passage is quoted in the Molo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6082</th>\n",
       "      <td>6086</td>\n",
       "      <td>Thank you for experimenting with  Wikipedia. Y...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text  toxic\n",
       "6078        6078  \"\\n\\nPenis envy\\nFrom Wikipedia, the free ency...      1\n",
       "6079        6079  Hrafn \\n\\nPM, Where did Hrafn go? There was a ...      0\n",
       "6080        6084  \"::I'll alos be looking in to see how this is ...      0\n",
       "6081        6085  \"\\n\\nThe Ezekiel passage is quoted in the Molo...      0\n",
       "6082        6086  Thank you for experimenting with  Wikipedia. Y...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[6078:6082]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь видно, что на индексе 6080 значение колонки unnamed уже не равно номеру индекса. Эту колонку можно удалить, так как она никак не поможет обучению модели. Это было и так понятно, но я решила проверить, почему колонка похожа на индексы, но отличается от них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=data.columns[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Информация о количестве данных, пропусках и типах данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество дубликатов: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Количество дубликатов:\", data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Промежуточный вывод:** удалена колонка Unnamed: 0, данные проверены на дубликаты и пропуски - их нет. Также просмотрены типы данных, все в порядке. Можно приступать к подготовке к обучению."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка к обучению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Баланс классов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть явный дисбаланс классов, я применю технику оверсемплинга для тренировочного набора данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создам корпус слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаю лемматизацию английского текста с помощью WordNetLemmatizer и POS-тегов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Катя\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Катя\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Катя\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "Лематизированный текст: Hey man , I 'm really not try to edit war . It 's just that this guy be constantly remove relevant information and talk to me through edits instead of my talk page . He seem to care more about the formatting than the actual info .\n",
      "Wall time: 12min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Если тег не распознается\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    tagged_words = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(pos_tag)) for word, pos_tag in tagged_words]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "corpus = pd.Series(corpus)\n",
    "\n",
    "lemmatized_corpus = corpus.apply(lemmatize_text)\n",
    "\n",
    "print(\"Исходный текст:\", corpus[2])\n",
    "print(\"Лематизированный текст:\", lemmatized_corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уберу лишние символы и приведу массив к нижнему регистру:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\n",
      "Очищенный, лемматизированный текст: hey man i m really not try to edit war it s just that this guy be constantly remove relevant information and talk to me through edits instead of my talk page he seem to care more about the formatting than the actual info\n",
      "Wall time: 4.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def clear_text(text):\n",
    "    edited = re.sub(r'[^a-zA-Z ]', ' ', text).split()\n",
    "    return \" \".join(edited)\n",
    "\n",
    "def lowercase_text(text):\n",
    "    lowercase_words = [word.lower() for word in text.split()]\n",
    "    return \" \".join(lowercase_words)\n",
    "\n",
    "cleared_corpus = lemmatized_corpus.apply(clear_text)\n",
    "lowercased_corpus = cleared_corpus.apply(lowercase_text)\n",
    "\n",
    "print(\"Исходный текст:\", corpus[2])\n",
    "print(\"Очищенный, лемматизированный текст:\", lowercased_corpus[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделю данные на train и test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размеры выборок:\n",
      "Train: 127433 127433\n",
      "Test: 31859 31859\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(lowercased_corpus, data['toxic'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Проверка\n",
    "print(\"Размеры выборок:\")\n",
    "print(\"Train:\", x_train.shape[0], y_train.shape[0])\n",
    "print(\"Test:\", x_test.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваю стоп-слова и присваиваю их переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Катя\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords') #скачивание стоп-слов из библиотеки\n",
    "\n",
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаю пайплайны для 3 моделей и делаю для них сетку гиперпараметров. Буду использовать векторайзер, также tf-idf трансформер, smote балансировщик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание пайплайна для каждой модели\n",
    "logreg_pipe = make_pipeline(\n",
    "    CountVectorizer(ngram_range=(1, 2), stop_words=english_stopwords),\n",
    "    TfidfTransformer(),\n",
    "    SMOTE(random_state=42),\n",
    "    LogisticRegression(random_state=42)\n",
    ")\n",
    "\n",
    "rf_pipe = make_pipeline(\n",
    "    CountVectorizer(ngram_range=(1, 2), stop_words=english_stopwords),\n",
    "    TfidfTransformer(),\n",
    "    SMOTE(random_state=42),\n",
    "    RandomForestClassifier(random_state=42)\n",
    ")\n",
    "\n",
    "lgbm_pipe = make_pipeline(\n",
    "    CountVectorizer(ngram_range=(1, 2), stop_words=english_stopwords),\n",
    "    TfidfTransformer(),\n",
    "    SMOTE(random_state=42),\n",
    "    LGBMClassifier(random_state=42)\n",
    ")\n",
    "\n",
    "\n",
    "# Создание сетки параметров для каждой модели\n",
    "logreg_params = {\n",
    "    'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'logisticregression__solver': ['liblinear', 'saga'],\n",
    "    'logisticregression__max_iter': range(100, 1000, 100),\n",
    "    'logisticregression__fit_intercept': [True, False],\n",
    "    'logisticregression__penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "\n",
    "rf_params = {\n",
    "    'randomforestclassifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "    'randomforestclassifier__criterion': ['gini', 'entropy'],\n",
    "    'randomforestclassifier__max_depth': [None, 5, 10, 20, 30],\n",
    "    'randomforestclassifier__min_samples_split': [2, 5, 10],\n",
    "    'randomforestclassifier__min_samples_leaf': [1, 2, 4],\n",
    "    'randomforestclassifier__max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'randomforestclassifier__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "lgbm_params = {\n",
    "    'lgbmclassifier__n_estimators': range(500, 1001, 100),\n",
    "    'lgbmclassifier__max_depth': range(5, 16),\n",
    "    'lgbmclassifier__learning_rate': [0.01, 0.1, 0.5, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаю метрику для LogisticRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Лучшие гиперпараметры LogisticRegression:  {'logisticregression__solver': 'liblinear', 'logisticregression__penalty': 'l2', 'logisticregression__max_iter': 500, 'logisticregression__fit_intercept': False, 'logisticregression__C': 100}\n",
      "Лучшая F1-мера:  0.7608801004373702\n"
     ]
    }
   ],
   "source": [
    "logreg_grid = RandomizedSearchCV(logreg_pipe, logreg_params, cv=3, scoring='f1', verbose=10, n_jobs=-1, random_state=42)\n",
    "\n",
    "logreg_grid.fit(x_train, y_train)\n",
    "print(\"Лучшие гиперпараметры LogisticRegression: \", logreg_grid.best_params_)\n",
    "print(\"Лучшая F1-мера: \", logreg_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаю метрику для RandomForestClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Лучшие гиперпараметры RandomForestClassifier:  {'randomforestclassifier__n_estimators': 400, 'randomforestclassifier__min_samples_split': 5, 'randomforestclassifier__min_samples_leaf': 2, 'randomforestclassifier__max_features': 'auto', 'randomforestclassifier__max_depth': 30, 'randomforestclassifier__criterion': 'entropy', 'randomforestclassifier__bootstrap': True}\n",
      "Лучшая F1-мера:  0.38499067561673206\n"
     ]
    }
   ],
   "source": [
    "rf_grid = RandomizedSearchCV(rf_pipe, rf_params, cv=3, scoring='f1', verbose=10, n_jobs=-1, random_state=42)\n",
    "\n",
    "rf_grid.fit(x_train, y_train)\n",
    "print(\"Лучшие гиперпараметры RandomForestClassifier: \", rf_grid.best_params_)\n",
    "print(\"Лучшая F1-мера: \", rf_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаю метрику для LGBMClassifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Лучшие гиперпараметры LGBMClassifier:  {'lgbmclassifier__n_estimators': 900, 'lgbmclassifier__max_depth': 12, 'lgbmclassifier__learning_rate': 0.5}\n",
      "Лучшая F1-мера:  0.7529564425229078\n"
     ]
    }
   ],
   "source": [
    "lgbm_grid = RandomizedSearchCV(lgbm_pipe, lgbm_params, cv=3, scoring='f1', verbose=10, n_jobs=-1, random_state=42)\n",
    "\n",
    "lgbm_grid.fit(x_train, y_train)\n",
    "print(\"Лучшие гиперпараметры LGBMClassifier: \", lgbm_grid.best_params_)\n",
    "print(\"Лучшая F1-мера: \", lgbm_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Лучшая модель** - логистическая регрессия с параметрами 'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 500, 'fit_intercept': False, 'C': 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование лучшей модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogisticRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на тесте: 0.7549497847919656\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg_grid.best_estimator_.predict(x_test)\n",
    "print(\"F1 на тесте:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка на адекватность с предсказанием классов в соответствии с их относительными частотами в тренировочных данных:\n",
    "\n",
    "Для данных не нужно применять трансформер или векторайзер, потому что по сути предсказание никак не зависит от признаков. Оно строится только на таргете со случайным предсказанием классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 на Dummy: 0.09324009324009325\n"
     ]
    }
   ],
   "source": [
    "model_dm = DummyClassifier(strategy='stratified', random_state=42)\n",
    "\n",
    "model_dm.fit(x_train, y_train)\n",
    "\n",
    "predicted_dm = model_dm.predict(x_test)\n",
    "\n",
    "print(\"F1 на Dummy:\", f1_score(y_test, predicted_dm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат F1 модели значительно лучше, чем результат Dummy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Данные были загружены и проверены, удалена одна колонка.\n",
    "2. Данные были изучены и подготовлены для обучения с помощью лемматизации, приведения к нижнему регистру, удалению стоп-слов, а также созданию мешка слов.\n",
    "3. Рассчитаны гиперпараметры для 3 моделей: логистическая регрессия, случайный лес, lgdm классификатор. Лучше всего себя проявила логистическая регрессия с гиперпараметрами 'solver': 'liblinear', 'penalty': 'l2', 'max_iter': 500, 'fit_intercept': False, 'C': 100.\n",
    "4. Лучшая модель проверена на тесте. Результат F1 = 0.755\n",
    "5. Проведена проверка на адекватность Dummy классификатором. F1 = 0.093\n",
    "\n",
    "**Выбранная модель:** LogisticRegression(random_state=42, solver='liblinear', penalty='l2', max_iter=500, fit_intercept=False, C=100). Тестирование проведено успешно."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 1545,
    "start_time": "2023-06-28T18:58:01.517Z"
   },
   {
    "duration": 112,
    "start_time": "2023-06-28T18:58:29.602Z"
   },
   {
    "duration": 2335,
    "start_time": "2023-06-28T18:58:57.201Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-28T18:59:09.031Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-28T18:59:22.557Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-28T18:59:40.316Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-28T18:59:47.090Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-28T19:00:04.298Z"
   },
   {
    "duration": 31,
    "start_time": "2023-06-28T19:00:18.517Z"
   },
   {
    "duration": 213,
    "start_time": "2023-06-28T19:00:25.157Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-28T19:01:09.244Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-28T19:04:44.701Z"
   },
   {
    "duration": 2689,
    "start_time": "2023-06-28T19:05:33.237Z"
   },
   {
    "duration": 46,
    "start_time": "2023-06-28T19:07:10.591Z"
   },
   {
    "duration": 1365,
    "start_time": "2023-06-28T19:07:19.988Z"
   },
   {
    "duration": 2348,
    "start_time": "2023-06-28T19:07:21.355Z"
   },
   {
    "duration": 11,
    "start_time": "2023-06-28T19:07:23.704Z"
   },
   {
    "duration": 15,
    "start_time": "2023-06-28T19:07:23.717Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-28T19:07:23.734Z"
   },
   {
    "duration": 39,
    "start_time": "2023-06-28T19:07:23.746Z"
   },
   {
    "duration": 20,
    "start_time": "2023-06-28T19:07:23.786Z"
   },
   {
    "duration": 38,
    "start_time": "2023-06-28T19:07:23.808Z"
   },
   {
    "duration": 204,
    "start_time": "2023-06-28T19:07:23.848Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-28T19:07:24.054Z"
   },
   {
    "duration": 2638,
    "start_time": "2023-06-28T19:07:24.061Z"
   },
   {
    "duration": 151,
    "start_time": "2023-06-28T19:09:35.688Z"
   },
   {
    "duration": 1488,
    "start_time": "2023-06-28T19:10:10.730Z"
   },
   {
    "duration": 2255,
    "start_time": "2023-06-28T19:10:12.220Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-28T19:10:14.476Z"
   },
   {
    "duration": 11,
    "start_time": "2023-06-28T19:10:14.487Z"
   },
   {
    "duration": 9,
    "start_time": "2023-06-28T19:10:14.500Z"
   },
   {
    "duration": 21,
    "start_time": "2023-06-28T19:10:14.511Z"
   },
   {
    "duration": 14,
    "start_time": "2023-06-28T19:10:14.534Z"
   },
   {
    "duration": 30,
    "start_time": "2023-06-28T19:10:14.549Z"
   },
   {
    "duration": 210,
    "start_time": "2023-06-28T19:10:14.581Z"
   },
   {
    "duration": 7,
    "start_time": "2023-06-28T19:10:14.793Z"
   },
   {
    "duration": 3108,
    "start_time": "2023-06-28T19:10:14.801Z"
   },
   {
    "duration": 46,
    "start_time": "2023-06-28T19:11:20.361Z"
   },
   {
    "duration": 1578,
    "start_time": "2023-06-28T19:11:24.441Z"
   },
   {
    "duration": 2263,
    "start_time": "2023-06-28T19:11:26.021Z"
   },
   {
    "duration": 12,
    "start_time": "2023-06-28T19:11:28.285Z"
   },
   {
    "duration": 15,
    "start_time": "2023-06-28T19:11:28.298Z"
   },
   {
    "duration": 14,
    "start_time": "2023-06-28T19:11:28.315Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-28T19:11:28.331Z"
   },
   {
    "duration": 13,
    "start_time": "2023-06-28T19:11:28.342Z"
   },
   {
    "duration": 39,
    "start_time": "2023-06-28T19:11:28.357Z"
   },
   {
    "duration": 225,
    "start_time": "2023-06-28T19:11:28.397Z"
   },
   {
    "duration": 6,
    "start_time": "2023-06-28T19:11:28.631Z"
   },
   {
    "duration": 3098,
    "start_time": "2023-06-28T19:11:30.792Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-28T19:12:29.695Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-28T19:13:24.810Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-28T19:14:55.995Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-28T19:16:34.788Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-28T19:18:22.058Z"
   },
   {
    "duration": 82,
    "start_time": "2023-06-28T19:18:36.529Z"
   },
   {
    "duration": 12,
    "start_time": "2023-06-28T19:18:59.983Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-28T19:19:12.658Z"
   },
   {
    "duration": 18496,
    "start_time": "2023-06-28T19:19:23.011Z"
   },
   {
    "duration": 235,
    "start_time": "2023-06-29T06:14:44.141Z"
   },
   {
    "duration": 209,
    "start_time": "2023-06-29T06:20:13.763Z"
   },
   {
    "duration": 2069,
    "start_time": "2023-06-29T06:34:59.081Z"
   },
   {
    "duration": 3285,
    "start_time": "2023-06-29T06:35:01.152Z"
   },
   {
    "duration": 13,
    "start_time": "2023-06-29T06:35:04.447Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-29T06:35:04.462Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-29T06:35:04.469Z"
   },
   {
    "duration": 11,
    "start_time": "2023-06-29T06:35:04.478Z"
   },
   {
    "duration": 50,
    "start_time": "2023-06-29T06:35:04.491Z"
   },
   {
    "duration": 44,
    "start_time": "2023-06-29T06:35:04.543Z"
   },
   {
    "duration": 284,
    "start_time": "2023-06-29T06:35:04.588Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-29T06:35:04.874Z"
   },
   {
    "duration": 1860,
    "start_time": "2023-06-29T06:35:04.881Z"
   },
   {
    "duration": 346,
    "start_time": "2023-06-29T06:35:06.745Z"
   },
   {
    "duration": 257,
    "start_time": "2023-06-29T06:35:07.092Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-29T06:36:08.549Z"
   },
   {
    "duration": 4521,
    "start_time": "2023-06-29T06:36:25.525Z"
   },
   {
    "duration": 3,
    "start_time": "2023-06-29T06:37:24.889Z"
   },
   {
    "duration": 39599,
    "start_time": "2023-06-29T06:37:30.175Z"
   },
   {
    "duration": 6271,
    "start_time": "2023-06-29T06:38:50.882Z"
   },
   {
    "duration": 4070,
    "start_time": "2023-06-29T06:39:33.882Z"
   },
   {
    "duration": 10161,
    "start_time": "2023-06-29T06:46:11.084Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-29T06:46:58.731Z"
   },
   {
    "duration": 19179,
    "start_time": "2023-06-29T06:47:04.450Z"
   },
   {
    "duration": 4,
    "start_time": "2023-06-29T06:48:48.587Z"
   },
   {
    "duration": 1922,
    "start_time": "2023-06-29T06:53:56.147Z"
   },
   {
    "duration": 1110,
    "start_time": "2023-06-29T06:53:58.071Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-29T06:53:59.183Z"
   },
   {
    "duration": 64,
    "start_time": "2023-06-29T06:53:59.195Z"
   },
   {
    "duration": 10,
    "start_time": "2023-06-29T06:53:59.261Z"
   },
   {
    "duration": 65,
    "start_time": "2023-06-29T06:53:59.273Z"
   },
   {
    "duration": 73,
    "start_time": "2023-06-29T06:53:59.340Z"
   },
   {
    "duration": 155,
    "start_time": "2023-06-29T06:53:59.414Z"
   },
   {
    "duration": 407,
    "start_time": "2023-06-29T06:53:59.581Z"
   },
   {
    "duration": 48,
    "start_time": "2023-06-29T06:53:59.991Z"
   },
   {
    "duration": 2394,
    "start_time": "2023-06-29T06:54:00.050Z"
   },
   {
    "duration": 50026,
    "start_time": "2023-06-29T06:54:02.458Z"
   },
   {
    "duration": 6490,
    "start_time": "2023-06-29T06:55:07.292Z"
   },
   {
    "duration": 23051,
    "start_time": "2023-06-29T06:55:31.018Z"
   },
   {
    "duration": 244,
    "start_time": "2023-06-29T07:00:27.230Z"
   },
   {
    "duration": 15,
    "start_time": "2023-06-29T07:00:50.894Z"
   },
   {
    "duration": 350,
    "start_time": "2023-06-29T07:00:54.344Z"
   },
   {
    "duration": 72,
    "start_time": "2023-06-29T07:06:45.220Z"
   },
   {
    "duration": 1991,
    "start_time": "2023-06-29T07:07:01.442Z"
   },
   {
    "duration": 983,
    "start_time": "2023-06-29T07:07:03.437Z"
   },
   {
    "duration": 15,
    "start_time": "2023-06-29T07:07:04.426Z"
   },
   {
    "duration": 5,
    "start_time": "2023-06-29T07:07:04.442Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-29T07:07:04.449Z"
   },
   {
    "duration": 12,
    "start_time": "2023-06-29T07:07:04.459Z"
   },
   {
    "duration": 13,
    "start_time": "2023-06-29T07:07:04.473Z"
   },
   {
    "duration": 41,
    "start_time": "2023-06-29T07:07:04.488Z"
   },
   {
    "duration": 303,
    "start_time": "2023-06-29T07:07:04.530Z"
   },
   {
    "duration": 8,
    "start_time": "2023-06-29T07:07:04.835Z"
   },
   {
    "duration": 1939,
    "start_time": "2023-06-29T07:07:04.844Z"
   },
   {
    "duration": 41357,
    "start_time": "2023-06-29T07:07:06.785Z"
   },
   {
    "duration": 5080,
    "start_time": "2023-06-29T07:07:48.153Z"
   },
   {
    "duration": 15755,
    "start_time": "2023-06-29T07:07:53.235Z"
   },
   {
    "duration": 51,
    "start_time": "2023-06-29T07:08:08.991Z"
   },
   {
    "duration": 369,
    "start_time": "2023-06-29T07:16:35.064Z"
   },
   {
    "duration": 208,
    "start_time": "2023-06-29T07:19:43.373Z"
   },
   {
    "duration": 28,
    "start_time": "2023-06-29T07:20:57.427Z"
   },
   {
    "duration": 33,
    "start_time": "2023-06-29T07:21:25.126Z"
   },
   {
    "duration": 29,
    "start_time": "2023-06-29T07:21:40.652Z"
   },
   {
    "duration": 30,
    "start_time": "2023-06-29T07:22:17.652Z"
   },
   {
    "duration": 38,
    "start_time": "2023-06-30T20:17:55.714Z"
   },
   {
    "duration": 3257,
    "start_time": "2023-07-03T09:27:35.248Z"
   },
   {
    "duration": 12026,
    "start_time": "2023-07-03T09:28:58.942Z"
   },
   {
    "duration": 37,
    "start_time": "2023-07-03T09:29:23.213Z"
   },
   {
    "duration": 1910,
    "start_time": "2023-07-03T09:31:29.698Z"
   },
   {
    "duration": 9111,
    "start_time": "2023-07-03T09:33:18.131Z"
   },
   {
    "duration": 1863,
    "start_time": "2023-07-03T09:33:37.252Z"
   },
   {
    "duration": 2647,
    "start_time": "2023-07-03T09:35:22.554Z"
   },
   {
    "duration": 1724,
    "start_time": "2023-07-03T09:35:28.321Z"
   },
   {
    "duration": 9485,
    "start_time": "2023-07-03T09:36:05.155Z"
   },
   {
    "duration": 1488,
    "start_time": "2023-07-03T09:36:21.894Z"
   },
   {
    "duration": 844,
    "start_time": "2023-07-03T09:43:58.609Z"
   },
   {
    "duration": 17,
    "start_time": "2023-07-03T09:43:59.725Z"
   },
   {
    "duration": 6,
    "start_time": "2023-07-03T09:44:02.521Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-03T09:44:03.716Z"
   },
   {
    "duration": 9,
    "start_time": "2023-07-03T09:44:04.403Z"
   },
   {
    "duration": 16,
    "start_time": "2023-07-03T09:44:07.673Z"
   },
   {
    "duration": 43,
    "start_time": "2023-07-03T09:44:08.656Z"
   },
   {
    "duration": 246,
    "start_time": "2023-07-03T09:44:10.241Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
